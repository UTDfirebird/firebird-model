# -*- coding: utf-8 -*-
"""TensorProduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTib_JLNFdXkn9L5OgCfGhse1YJxiBY4
"""

import os
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define input and output directories
baseDir = "/content/drive/MyDrive/Model Development/"
inputDir = os.path.join(baseDir, "ProcessedCSV")
outputDir = os.path.join(baseDir, "ProcessedTensors")

# Ensure output directory exists in Google Drive
os.makedirs(outputDir, exist_ok=True)

# Initialize DistilBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# tokenize tweets
def tokenizeTweets(text):
    return tokenizer(text, padding="max_length", truncation=True, max_length=128, return_tensors="pt")

# Process all CSV files in the directory
for fileName in os.listdir(inputDir):
    if fileName.endswith(".csv"):
        filePath = os.path.join(inputDir, fileName)
        print(f"Processing: {filePath}")

        # load preprocessed CSV
        df = pd.read_csv(filePath)

        # Ensure necessary columns exist
        if "cleaned_tweet" not in df.columns or "class_label" not in df.columns:
            print(f"Skipping {fileName} - Required columns not found.")
            continue

        # apply tokenization to all tweets
        encodings = df["cleaned_tweet"].apply(lambda x: tokenizeTweets(x)).tolist()

        # convert tokenized inputs to tensors
        input_ids = torch.cat([e["input_ids"] for e in encodings])
        attention_mask = torch.cat([e["attention_mask"] for e in encodings])

        # convert class labels to numerical format
        labelMapping = {label: i for i, label in enumerate(df["class_label"].unique())}
        df["label"] = df["class_label"].map(labelMapping)

        # convert labels to tensor
        labels = torch.tensor(df["label"].values)

        # split into train (80%) and validation (20%) sets
        trainInputs, valInputs, trainLabels, valLabels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)
        trainMasks, valMasks, _, _ = train_test_split(attention_mask, attention_mask, test_size=0.2, random_state=42)

        # convert to torch datasets
        trainData = torch.utils.data.TensorDataset(trainInputs, trainMasks, trainLabels)
        valData = torch.utils.data.TensorDataset(valInputs, valMasks, valLabels)

        # Define save paths in Google Drive
        baseName = fileName.replace(".csv", "")  # Remove .csv extension
        trainSavePath = os.path.join(outputDir, f"{baseName}_train.pt")
        valSavePath = os.path.join(outputDir, f"{baseName}_val.pt")

        # Save processed tensors for training
        torch.save(trainData, trainSavePath)
        torch.save(valData, valSavePath)

        print(f"Saved: {trainSavePath} and {valSavePath}")

print("Processing complete. All datasets saved to Google Drive.")

import os
print(os.listdir())